{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6l931wwSFVm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from PIL import UnidentifiedImageError"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/siriraj-eye-dataset/Image/all_images-001.zip"
      ],
      "metadata": {
        "id": "egU_v_9_cIrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '/content/drive/MyDrive/siriraj-eye-dataset/all_labels_processed.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "df = df.drop(columns=['camera','glaucoma_suspect','image_quality','image_quality_split', 'has_cup', 'has_disc', 'filename', 'has_cup_split', 'has_disc_split'])\n",
        "\n",
        "old_base_path = '../dataset/siriraj-eye-dataset-2023-jan/all_images'\n",
        "new_base_path = '/content/all_images'\n",
        "df['path'] = df['path'].apply(lambda x: x.replace(old_base_path, new_base_path).replace('\\\\', '/'))"
      ],
      "metadata": {
        "id": "lf-j6KaUcJ3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index('path', inplace=False)"
      ],
      "metadata": {
        "id": "K7dZoZ4ccLmS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "45ed215b-ecd5-4368-c239-3f1b47f3bc8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  dr_label dr_label_split\n",
              "path                                                     \n",
              "/content/all_images/0.jpg              0.0          train\n",
              "/content/all_images/1.jpg              0.0          train\n",
              "/content/all_images/2.jpg              0.0          train\n",
              "/content/all_images/3.jpg              0.0          train\n",
              "/content/all_images/4.jpg              0.0          train\n",
              "...                                    ...            ...\n",
              "/content/all_images/515L (C).jpg       NaN            NaN\n",
              "/content/all_images/596L (c).jpg       NaN            NaN\n",
              "/content/all_images/544L (C).jpg       NaN            NaN\n",
              "/content/all_images/538L (C).jpg       NaN            NaN\n",
              "/content/all_images/282L (C).jpg       NaN            NaN\n",
              "\n",
              "[6168 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86c7b61a-5afe-4833-8e80-1869162968e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dr_label</th>\n",
              "      <th>dr_label_split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>path</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>/content/all_images/0.jpg</th>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/all_images/1.jpg</th>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/all_images/2.jpg</th>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/all_images/3.jpg</th>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/all_images/4.jpg</th>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/all_images/515L (C).jpg</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/all_images/596L (c).jpg</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/all_images/544L (C).jpg</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/all_images/538L (C).jpg</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/all_images/282L (C).jpg</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6168 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86c7b61a-5afe-4833-8e80-1869162968e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86c7b61a-5afe-4833-8e80-1869162968e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86c7b61a-5afe-4833-8e80-1869162968e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['dr_label'])"
      ],
      "metadata": {
        "id": "wwDus0eWe8jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2wXT9hsgV8X",
        "outputId": "f869d705-de03-47e2-f010-099fb6b7df9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              path  dr_label dr_label_split\n",
            "0        /content/all_images/0.jpg       0.0          train\n",
            "1        /content/all_images/1.jpg       0.0          train\n",
            "2        /content/all_images/2.jpg       0.0          train\n",
            "3        /content/all_images/3.jpg       0.0          train\n",
            "4        /content/all_images/4.jpg       0.0          train\n",
            "...                            ...       ...            ...\n",
            "4612  /content/all_images/4612.jpg       1.0           test\n",
            "4613  /content/all_images/4613.jpg       1.0          train\n",
            "4614  /content/all_images/4614.jpg       1.0          train\n",
            "4615  /content/all_images/4615.jpg       1.0           test\n",
            "4616  /content/all_images/4616.jpg       1.0          train\n",
            "\n",
            "[4617 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df[df['dr_label_split'] == 'train']\n",
        "test_size = 924 / len(train_df)\n",
        "train_df, val_df = train_test_split(train_df, test_size=test_size, random_state=42, stratify=train_df['dr_label'])\n",
        "test_df = df[df['dr_label_split'] == 'test']\n",
        "train_df.drop(columns=['dr_label_split'], inplace=True)\n",
        "val_df.drop(columns=['dr_label_split'], inplace=True)\n",
        "test_df.drop(columns=['dr_label_split'], inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6rbzNSFgY1h",
        "outputId": "149b8fe4-ada5-4e51-adea-1d3944b60b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-e54866af1ffb>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df.drop(columns=['dr_label_split'], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AVr0mYTgf7R",
        "outputId": "4b9904d9-8d01-46fc-96ca-d1fa5b08a6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              path  dr_label\n",
            "29      /content/all_images/29.jpg       0.0\n",
            "30      /content/all_images/30.jpg       0.0\n",
            "32      /content/all_images/32.jpg       0.0\n",
            "43      /content/all_images/43.jpg       0.0\n",
            "44      /content/all_images/44.jpg       0.0\n",
            "...                            ...       ...\n",
            "4593  /content/all_images/4593.jpg       1.0\n",
            "4604  /content/all_images/4604.jpg       1.0\n",
            "4607  /content/all_images/4607.jpg       1.0\n",
            "4612  /content/all_images/4612.jpg       1.0\n",
            "4615  /content/all_images/4615.jpg       1.0\n",
            "\n",
            "[924 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (512, 512)\n",
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "a_FhniJ5iI24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      img_path, label = self.dataframe.iloc[index]\n",
        "      try:\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "      except UnidentifiedImageError:\n",
        "        print(f\"Skipping corrupted or unsupported image: {img_path}\")\n",
        "        image = Image.new('RGB', (512, 512), color='white')\n",
        "      image = self.transform(image)\n",
        "      return image, label\n"
      ],
      "metadata": {
        "id": "r-AEtSmdir7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = val_transform"
      ],
      "metadata": {
        "id": "69taVyxEi4pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_df, transform=train_transform)\n",
        "val_dataset = CustomDataset(val_df, transform=val_transform)\n",
        "test_dataset = CustomDataset(test_df, transform=test_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "id": "CQROECzhjduV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        self.model.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        probas = nn.functional.softmax(x, dim=1)\n",
        "        return x, probas\n"
      ],
      "metadata": {
        "id": "aexvWGGsjhW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Net().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTsknxtBj8xh",
        "outputId": "83f3e2fc-28a3-4e89-d759-a5d29f8188e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 152MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "nWUa7ZC8j_Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in data_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, probas = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(probas.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / len(data_loader), correct / total\n"
      ],
      "metadata": {
        "id": "7oPHW2ZWkAyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            logits, probas = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(probas.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / len(data_loader), correct / total\n"
      ],
      "metadata": {
        "id": "z9ELTkLIkKnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = 1.0\n",
        "best_model_path = '/content/drive/MyDrive/siriraj-eye-dataset/dr_model.pth'\n",
        "patience = 5\n",
        "wait = 0"
      ],
      "metadata": {
        "id": "44rE8DWmkL0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        print(f'New best model found! Previous best: {best_val_loss:.4f}, new best: {val_loss:.4f}. Saving model...')\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        best_val_loss = val_loss\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "\n",
        "    if wait > patience:\n",
        "        print(f'Early stopping after {epoch+1} epochs')\n",
        "        break"
      ],
      "metadata": {
        "id": "hhvvZ3NbkPwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/siriraj-eye-dataset/dr_model.pth'))\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            probas, outputs = model(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['negative', 'positive'], yticklabels=['negative', 'positive'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "test_accuracy = evaluate(model, test_loader, device)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "xxrPNbQ6kZRY",
        "outputId": "b99cc2c2-90cd-4e12-d628-89fd427d04bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "\n",
        "def predict_single_image(model, image_path, device):\n",
        "    transform = Compose([\n",
        "        Resize(IMG_SIZE),\n",
        "        CenterCrop(IMG_SIZE),\n",
        "        ToTensor(),\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, probas = model(image)\n",
        "        probas = probas.cpu().numpy()\n",
        "    return probas\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-LQB9fryRsf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/all_images/161R (2).jpg'\n",
        "probas = predict_single_image(model, image_path, device)\n",
        "print(f'Negative: {probas[0][0]:.2f}, Positive: {probas[0][1]:.2f}')"
      ],
      "metadata": {
        "id": "MrYOl-_iRs6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDw1AMeWR1CN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}